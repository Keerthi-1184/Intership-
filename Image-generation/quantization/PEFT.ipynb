{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fea14e55",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'clear_device_cache' from 'accelerate.utils.memory' (c:\\Users\\SreeKeerthiReddyThat\\Desktop\\Image-generation\\venv\\Lib\\site-packages\\accelerate\\utils\\memory.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SreeKeerthiReddyThat\\Desktop\\Image-generation\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1364\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1363\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\Lib\\importlib\\__init__.py:90\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     89\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1387\u001b[39m, in \u001b[36m_gcd_import\u001b[39m\u001b[34m(name, package, level)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:995\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:488\u001b[39m, in \u001b[36m_call_with_frames_removed\u001b[39m\u001b[34m(f, *args, **kwds)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SreeKeerthiReddyThat\\Desktop\\Image-generation\\venv\\Lib\\site-packages\\transformers\\trainer.py:190\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_peft_available():\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[32m    193\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_accelerate_available():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SreeKeerthiReddyThat\\Desktop\\Image-generation\\venv\\Lib\\site-packages\\peft\\__init__.py:17\u001b[39m\n\u001b[32m     15\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m0.15.2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mauto\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     MODEL_TYPE_TO_PEFT_MODEL_MAPPING,\n\u001b[32m     19\u001b[39m     AutoPeftModel,\n\u001b[32m     20\u001b[39m     AutoPeftModelForCausalLM,\n\u001b[32m     21\u001b[39m     AutoPeftModelForFeatureExtraction,\n\u001b[32m     22\u001b[39m     AutoPeftModelForQuestionAnswering,\n\u001b[32m     23\u001b[39m     AutoPeftModelForSeq2SeqLM,\n\u001b[32m     24\u001b[39m     AutoPeftModelForSequenceClassification,\n\u001b[32m     25\u001b[39m     AutoPeftModelForTokenClassification,\n\u001b[32m     26\u001b[39m )\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig, PromptLearningConfig\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SreeKeerthiReddyThat\\Desktop\\Image-generation\\venv\\Lib\\site-packages\\peft\\auto.py:31\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     22\u001b[39m     AutoModel,\n\u001b[32m     23\u001b[39m     AutoModelForCausalLM,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     AutoTokenizer,\n\u001b[32m     29\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftConfig\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpeft_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     33\u001b[39m     PeftModel,\n\u001b[32m     34\u001b[39m     PeftModelForCausalLM,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     PeftModelForTokenClassification,\n\u001b[32m     40\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SreeKeerthiReddyThat\\Desktop\\Image-generation\\venv\\Lib\\site-packages\\peft\\config.py:24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PushToHubMixin\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CONFIG_NAME, PeftType, TaskType\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# we expect at least these keys to be present in a PEFT adapter_config.json\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SreeKeerthiReddyThat\\Desktop\\Image-generation\\venv\\Lib\\site-packages\\peft\\utils\\__init__.py:16\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m map_cache_to_layer_device_map\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mloftq_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m replace_lora_weights_loftq\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mother\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     18\u001b[39m     CONFIG_NAME,\n\u001b[32m     19\u001b[39m     INCLUDE_LINEAR_LAYERS_SHORTHAND,\n\u001b[32m   (...)\u001b[39m\u001b[32m     50\u001b[39m     transpose,\n\u001b[32m     51\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SreeKeerthiReddyThat\\Desktop\\Image-generation\\venv\\Lib\\site-packages\\peft\\utils\\loftq_utils.py:25\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01maccelerate\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m clear_device_cache\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m snapshot_download\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'clear_device_cache' from 'accelerate.utils.memory' (c:\\Users\\SreeKeerthiReddyThat\\Desktop\\Image-generation\\venv\\Lib\\site-packages\\accelerate\\utils\\memory.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgc\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     AutoModelForCausalLM,\n\u001b[32m      6\u001b[39m     AutoTokenizer,\n\u001b[32m      7\u001b[39m     TrainingArguments,\n\u001b[32m      8\u001b[39m     Trainer\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_peft_model, LoraConfig, TaskType\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SreeKeerthiReddyThat\\Desktop\\Image-generation\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1354\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1352\u001b[39m     value = \u001b[38;5;28mself\u001b[39m._get_module(name)\n\u001b[32m   1353\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module.keys():\n\u001b[32m-> \u001b[39m\u001b[32m1354\u001b[39m     module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1355\u001b[39m     value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   1356\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\SreeKeerthiReddyThat\\Desktop\\Image-generation\\venv\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1366\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m1366\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1367\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m because of the following error (look up to see its\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1369\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'clear_device_cache' from 'accelerate.utils.memory' (c:\\Users\\SreeKeerthiReddyThat\\Desktop\\Image-generation\\venv\\Lib\\site-packages\\accelerate\\utils\\memory.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from datasets import Dataset\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datetime import datetime\n",
    " \n",
    "# Unique folder names per run\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "orig_dir = f\"./models/orig_{timestamp}\"\n",
    "float16_dir = f\"./models/float16_{timestamp}\"\n",
    "peft_dir = f\"./models/peft_{timestamp}\"\n",
    "model_name = \"distilgpt2\"\n",
    " \n",
    "# Load tokenizer and set padding token if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Fix padding token issue\n",
    " \n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    " \n",
    "# Save original model/tokenizer\n",
    "model.save_pretrained(orig_dir)\n",
    "tokenizer.save_pretrained(orig_dir)\n",
    " \n",
    "# Print original size\n",
    "def get_size(path):\n",
    "    return sum(os.path.getsize(os.path.join(dp, f)) for dp, _, fn in os.walk(path) for f in fn) / (1024 ** 2)\n",
    " \n",
    "print(f\"Original model size: {get_size(orig_dir):.2f} MB\")\n",
    "print(\"Dtype before conversion:\", next(model.parameters()).dtype)\n",
    " \n",
    "# Convert to float16\n",
    "model.half()\n",
    "print(\"Dtype after conversion:\", next(model.parameters()).dtype)\n",
    " \n",
    "# Save float16 version\n",
    "model.save_pretrained(float16_dir)\n",
    "tokenizer.save_pretrained(float16_dir)\n",
    "print(f\"Float16 model size: {get_size(float16_dir):.2f} MB\")\n",
    " \n",
    "# Reload float16 model (free memory first)\n",
    "del model\n",
    "gc.collect()\n",
    "model = AutoModelForCausalLM.from_pretrained(float16_dir, torch_dtype=torch.float16)\n",
    " \n",
    "# Apply LoRA PEFT config\n",
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"c_attn\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    " \n",
    "# Dummy dataset\n",
    "texts = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence is transforming the world.\",\n",
    "]\n",
    " \n",
    "inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=32, return_tensors=\"pt\")\n",
    " \n",
    "dataset = Dataset.from_dict({\n",
    "    \"input_ids\": inputs[\"input_ids\"],\n",
    "    \"attention_mask\": inputs[\"attention_mask\"]\n",
    "})\n",
    " \n",
    "# Data collator function for Trainer\n",
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([torch.tensor(item[\"input_ids\"]) for item in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(item[\"attention_mask\"]) for item in batch])\n",
    "    labels = torch.stack([torch.tensor(item[\"input_ids\"]) for item in batch])  # labels = input_ids for causal LM\n",
    " \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    " \n",
    " \n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./training_output/{timestamp}\",\n",
    "    per_device_train_batch_size=2,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=1,\n",
    "    save_steps=10,\n",
    "    fp16=True,\n",
    "    learning_rate=5e-4,\n",
    "    save_total_limit=1,\n",
    ")\n",
    " \n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    " \n",
    "# Train\n",
    "trainer.train()\n",
    " \n",
    "# Save LoRA fine-tuned model\n",
    "model.save_pretrained(peft_dir)\n",
    "tokenizer.save_pretrained(peft_dir)\n",
    "print(f\"LoRA fine-tuned model saved to: {peft_dir}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10af5813",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
