{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5da9c12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2e8174",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\"Content-Type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c323061",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= \"deepseek-r1:1.5b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe5af058",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_1 = \"Please generate test cases for chatbot and text summarizing agent \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b359d7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_2 = \"summarize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcf9b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": \"deepseek-r1:1.5b\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt_2 },\n",
    "            {\"role\": \"user\", \"content\": user_prompt_1}\n",
    "        ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ebee8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.post(\"http://localhost:11434/v1/chat/completions\", json=payload, headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cd59639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I'm trying to figure out how to create some test cases for a chatbot or a text summarizing agent. The user provided some example test cases for both, but I want to make sure I understand everything correctly and can think critically about whether the test cases are valid.\n",
      "\n",
      "First, let me look at the test cases for the chatbot. It includes things like asking questions, responding with information, and handling errors. Hmm, that covers the main areas, but maybe it's a bit too broad. What if some questions aren't appropriate or could lead to unintended behaviors?\n",
      "\n",
      "Now, looking at the text summarizing agent test cases. They have a variety of scenarios, from writing summaries for resumes and documents to applying summarization techniques in AI/ML models. But wait, how specific are these tests? For example, does each test case cover one scenario or multiple related ones? That might make it seem too narrow.\n",
      "\n",
      "I wonder if the user intended to create a more comprehensive set of test cases. Perhaps instead of breaking down everything into separate categories like questions and responses, they could group them more cohesively based on different types of tasks. Like pre-summarization, application, evaluation, etc.\n",
      "\n",
      "Also, when creating tests for an AI/ML model, it's crucial to cover edge cases and real-world scenarios. But maybe in the context provided, those have been overlooked. Could there be situations where a summarizer would fail correctly in these test cases? Maybe some tests show incorrect summaries or misleading results.\n",
      "\n",
      "Another thought: some test cases might not differentiate between variables like source documents versus text passages. That could make comparisons difficult. Also, using the same type of input (like paragraph vs sentence) repeatedly can limit thorough testing unless varied enough.\n",
      "\n",
      "Perhaps I should consider how each test case is intended to be used and what outcomes are expected. Are these tests for unit testing, integration testing, or something else? If they're unit tests, maybe ensuring a variety in user inputs is better than just generic questions. Integration tests might be about combining an agent with a chatbot; covering different functionalities would make it stronger.\n",
      "\n",
      "I also recall that test coverage should relate to the functionality we want to ensure. For example, should each test case be related to one specific aspect (like extracting entities, summarizing for intent, etc.) or cover multiple points? Probably the former as long as all aspects are included.\n",
      "\n",
      "Moreover, user feedback is important in testing. Maybe some tests could include a brief analysis if we wanted more detailed evaluation than just pass/fail results. But without explicit instructions on that, it's hard to say.\n",
      "\n",
      "Overall, my thought process involves evaluating how broad and specific each test case is given the context of chatbot or summarizing agents. Perhaps enhancing them by grouping related tasks together, using varied inputs, checking for edge cases, and ensuring thorough coverage would solidify their effectiveness.\n",
      "</think>\n",
      "\n",
      "To create a more comprehensive and effective set of test cases for a chatbot or text summarizing agent, consider the following enhancements:\n",
      "\n",
      "1. **Grouping by Task**: Organize tests into groups based on specific tasks such as pre-summarization, application, evaluation, analysis, etc., to provide focused coverage.\n",
      "\n",
      "2. **Diversity in Input Types**: Include varied input types (e.g., paragraph vs. sentence, primary vs. secondary sources) and test different functionalities in both models and agents.\n",
      "\n",
      "3. **Edge Cases Handling**: Each test case should cover potential edge cases, such as invalid inputs or extreme scenarios, to ensure robustness against unexpected situations.\n",
      "\n",
      "4. **Thorough Coverage**: Ensure each task is tested for its intended purpose, providing specific outcomes that align with desired functionalities, without the limitation of generic questions.\n",
      "\n",
      "5. **Analysis Support**: Encourage test cases to include brief analyses if additional feedback is desired, enhancing the value of testing beyond pass/fail results.\n",
      "\n",
      "6. **User Feedback Integration**: Consider using comments or summaries in test feedback for detailed insights where appropriate.\n",
      "\n",
      "By integrating these elements, we enhance the validity and utility of our tests, ensuring they cover all critical aspects without becoming limited by broad categories like questions and responses.\n"
     ]
    }
   ],
   "source": [
    "a=response.json()['choices'][0]['message']['content']\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84110b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
